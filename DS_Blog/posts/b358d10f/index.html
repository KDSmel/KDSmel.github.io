
<!DOCTYPE html>

<html class="theme-next mist use-motion" lang>
<head><meta name="generator" content="Hexo 3.9.0">
<!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="UTF-8">

<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<link rel="stylesheet" href="/DS_Blog/lib/Han/dist/han.min.css?v=3.3">
<link rel="stylesheet" href="/DS_Blog/lib/fancybox/source/jquery.fancybox.css">
<link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/font-awesome/4.7.0/css/font-awesome.min.css">
<link rel="stylesheet" href="/DS_Blog/css//main.css?v=7.1.2">
<link rel="apple-touch-icon" sizes="180x180" href="/DS_Blog/images/apple-touch-icon-next.png?v=7.1.2">
<link rel="icon" type="image/png" sizes="32x32" href="/DS_Blog/images/favicon-32x32-next.png?v=7.1.2">
<link rel="icon" type="image/png" sizes="16x16" href="/DS_Blog/images/favicon-16x16-next.png?v=7.1.2">
<link rel="mask-icon" href="/DS_Blog/images/logo.svg?v=7.1.2" color="#222">

<script id="hexo.configurations">
var NexT = window.NexT || {};
var CONFIG = {
root: '/',
scheme: 'Mist',
version: '7.1.2',
sidebar: {"position":"right","display":"hide","offset":12,"onmobile":false,"dimmer":false},
back2top: true,
back2top_sidebar: false,
fancybox: true,
fastclick: true,
lazyload: true,
tabs: true,
motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
algolia: {
applicationID: '',
apiKey: '',
indexName: '',
hits: {"per_page":10},
labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
}
};
</script>

<meta name="description" content="linear function: \(h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2+...+θ_dx_d=θ^Tx\) cost function: \(J(θ)=\frac{1}{2}\sum_{i=1}^n(h_θ(x^{(i)})-y^{(i)})^2\)">
<meta name="keywords" content="Linear Regression,Logistic Regression,Regression">
<meta property="og:type" content="article">
<meta property="og:title" content="CS229 Note: Linear Regression, Logistic regression, Generalized Linear Models">
<meta property="og:url" content="https://kdsmel.github.io/DS_Blog/posts/9b3e7e9e/index.html">
<meta property="og:site_name" content="Kamel&#39;s Notes">
<meta property="og:description" content="linear function: \(h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2+...+θ_dx_d=θ^Tx\) cost function: \(J(θ)=\frac{1}{2}\sum_{i=1}^n(h_θ(x^{(i)})-y^{(i)})^2\)">
<meta property="og:locale" content="default">
<meta property="og:image" content="https://kdsmel.github.io/DS_Blog/posts/9b3e7e9e/sigmoid.png">
<meta property="og:image" content="https://kdsmel.github.io/DS_Blog/posts/9b3e7e9e/newton.png">
<meta property="og:updated_time" content="2019-10-23T06:07:40.457Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="CS229 Note: Linear Regression, Logistic regression, Generalized Linear Models">
<meta name="twitter:description" content="linear function: \(h_θ(x) = θ_0 + θ_1x_1 + θ_2x_2+...+θ_dx_d=θ^Tx\) cost function: \(J(θ)=\frac{1}{2}\sum_{i=1}^n(h_θ(x^{(i)})-y^{(i)})^2\)">
<meta name="twitter:image" content="https://kdsmel.github.io/posts/9b3e7e9e/sigmoid.png">

<link rel="alternate" href="/DS_Blog/atom.xml" title="Kamel's Notes" type="application/atom+xml">

<link rel="canonical" href="https://kdsmel.github.io/DS_Blog/posts/9b3e7e9e/">

<script id="page.configurations">
CONFIG.page = {
sidebar: "",
};
</script>

<title>CS229 Note: Linear Regression, Logistic regression, Generalized Linear Models | Kamel's Notes</title>

<noscript>
<style>
.use-motion .motion-element,
.use-motion .brand,
.use-motion .menu-item,
.sidebar-inner,
.use-motion .post-block,
.use-motion .pagination,
.use-motion .comments,
.use-motion .post-header,
.use-motion .post-body,
.use-motion .collection-title { opacity: initial; }

.use-motion .logo,
.use-motion .site-title,
.use-motion .site-subtitle {
opacity: initial;
top: initial;
}

.use-motion .logo-line-before i { left: initial; }
.use-motion .logo-line-after i { right: initial; }
</style>
</noscript><!-- hexo-inject:begin --><!-- hexo-inject:end -->

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="default">

<!-- hexo-inject:begin --><!-- hexo-inject:end --><div class="container sidebar-position-right page-post-detail">
<div class="headband"></div>

<header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
<div class="header-inner"><div class="site-brand-wrapper">
<div class="site-meta">


<div class="custom-logo-site-title">
<a href="/" class="brand" rel="start">
<span class="logo-line-before"><i></i></span>
<span class="site-title">Kamel's Notes</span>
<span class="logo-line-after"><i></i></span>
</a>
</div>
<p class="site-subtitle">Code changes world!</p>

</div>
<div class="site-nav-toggle">
<button aria-label="Toggle navigation bar">
<span class="btn-bar"></span>
<span class="btn-bar"></span>
<span class="btn-bar"></span>
</button>
</div>
</div>

<nav class="site-nav">
<ul id="menu" class="menu">
<li class="menu-item menu-item-home">

<a href="/DS_Blog/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

</li>

<li class="menu-item menu-item-ml">

<a href="/DS_Blog/categories/Machine-Learning" rel="section"><i class="menu-item-icon fa fa-fw fa-th"></i> <br>ML</a>
</li>

<li class="menu-item menu-item-big-data">
<a href="/DS_Blog/categories/Big-Data" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Big Data</a>
</li>
<li class="menu-item menu-item-journal">
<a href="/DS_Blog/categories/Journal/" rel="section"><i class="menu-item-icon fa fa-fw fa-coffee"></i> <br>Journal</a>
</li>
</ul>

</nav>

</div>
</header>
    

    
    
    
    <div class="post-body han-init-context" itemprop="articleBody">

      
      

      
        <p>Some notation: <span class="math display">\[
\begin{align}
\theta^Tx=\sum_{i=1}^n \theta_ix_i \tag{weighted sum} \\
\sigma(z)=\frac{1}{1+e^{-z}} \tag{sigmoid function}
\end{align}
\]</span> <a id="more"></a></p>
<h1 id="logistic-regression-overview">Logistic Regression Overview</h1>
<p><strong>Logistic Regression</strong> is a classification algorithm that works by trying to learn a function that approximates <span class="math inline">\(P(Y|X)\)</span>. It makes the central assumption that $ P(Y|X) $ can be <strong><em>approximated as a sigmoid function applied to a linear combination of input features</em></strong>(important!).</p>
<p>Mathematically, for a single training datapoint <span class="math inline">\((x, y)\)</span> Logistic Regression assumes: <span class="math display">\[
P(Y = 1|\mathbf{X = x}) = σ(z) \text{,  where } z = θ_0 +\sum_{i=1}^mθ_ix_i
\]</span> This assumption is often written in the equivalent forms: <span class="math display">\[
P(Y = 1|\mathbf{X = x}) = σ(\theta^T\mathbf{x}) \tag{where we alwyas set $x_0$ to be 1}\\
P(Y = 0|\mathbf{X = x}) = 1-σ(\theta^T\mathbf{x})
\]</span> Using these equations for probability of <span class="math inline">\(Y|X\)</span> we can create an algorithm that select values of <span class="math inline">\(\theta\)</span> that maximize that probability for all data.</p>
<h2 id="logistic-regression-assumption">Logistic Regression Assumption</h2>
<ol type="1">
<li>Binary logistic regression requires the dependent variable to be binary</li>
<li>Observations to be independent of each other. In other words, the observations should not come from repeated measurements or matched data.</li>
<li>Little or no <strong>multicollinearity</strong> among the independent variables. This means that the independent variables should not be too highly correlated with each other.</li>
<li>Linearity of independent variables and <strong>log odds</strong>.</li>
<li>Typically requires a large sample size.</li>
</ol>
<h2 id="log-likelihood">Log Likelihood</h2>
<p>To start, here is a super slick way of writing the <strong>probability of one datapoint</strong>: <span class="math display">\[
P(Y=y|X=\mathbf{x})=\sigma(\theta^T\mathbf{x})^y \cdot[1-\sigma(\theta^T\mathbf{x})]^{1-y}
\]</span> Since each datapoint is <em>independent</em>, the <strong>probability of all the data (likelihood)</strong> is: <span class="math display">\[
\begin{align}
l(\theta)&amp;=\prod_{i=1}^n P(Y=y^{(i)}| X=\mathbf{x}^{(i)}) \\
&amp;=\prod_{i=1}^n \sigma(\theta^T\mathbf{x}^{(i)})^{y^{(i)}} \cdot[1-\sigma(\theta^T\mathbf{x})^{(i)}]^{1-y^{(i)}}
\end{align}
\]</span> Take the log, then you get the <strong>Log Likelihood</strong> for Logistic Regression. <span class="math display">\[
ll(\theta)=\sum_{i=1}^n y^{(i)}\log{\sigma(\theta^T\mathbf{x}^{(i)})}+(1-y^{(i)})\log{[1-\sigma(\theta^T\mathbf{x}^{(i)})]}
\]</span> <strong>Odds</strong> <span class="math display">\[
\frac{\sigma(z)}{1-\sigma(z)}=\frac{y}{1-y}=e^{\theta^T\mathbf{x}}
\]</span> Take on any value between 0 and <span class="math inline">\(∞\)</span>.</p>
<p><strong>Log-odds (Logit)</strong> <span class="math display">\[
\begin{align}
\log{\frac{\sigma(z)}{1-\sigma(z)}}=\log{\frac{y}{1-y}} =\theta ^T\mathbf{x}
\end{align}
\]</span></p>
<ul>
<li><strong>logit</strong> is linear in <span class="math inline">\(\mathbf{x}\)</span>.
<ul>
<li><p>There is not a straight-line relationship between <span class="math inline">\(\sigma(\theta^T \mathbf{x})\)</span> and <span class="math inline">\(\mathbf{x}\)</span>,</p></li>
<li><p>The rate of change in <span class="math inline">\(\sigma(\theta^T \mathbf{x})\)</span> per unit change in <span class="math inline">\(X\)</span> depends on the current value of $ $.</p></li>
</ul></li>
</ul>
<h2 id="gradient-of-log-likelihood">Gradient of Log Likelihood</h2>
<p>Now that we have a function for <strong>log-likelihood</strong>, we simply need to chose the values of <span class="math inline">\(\theta\)</span> that maximize it.</p>
<p>To start, here is the definition for the derivative of sigma with respect to its inputs: <span class="math display">\[
\frac{\partial}{\partial z}\sigma(z)=\sigma(z)[1-\sigma(z)] 
\]</span></p>
<p>Here is the partial derivative of log-likelihood with respect to each parameter <span class="math inline">\(θ_j\)</span>: <span class="math display">\[
\begin{align}
\frac{\partial ll(\theta)}{\partial\theta_j}&amp;=\frac{\partial}{\partial \theta_j}y\log{\sigma(\theta^T\mathbf{x})} +\frac{\partial}{\partial \theta_j}(1-y)\log{[1-\sigma(\theta^T\mathbf{x})]} \\
&amp;=[\frac{y}{\sigma(\theta^Tx)}-\frac{1-y}{1-\sigma(\theta^Tx)}] \frac{\partial}{\partial \theta_j}\sigma(\theta^Tx)  \\
&amp;=[\frac{y}{\sigma(\theta^Tx)}-\frac{1-y}{1-\sigma(\theta^Tx)} ]\sigma(\theta^Tx)[1-\sigma(\theta^Tx)]x_j \\
&amp;=\frac{y-\sigma(\theta^Tx)}{\sigma(\theta^Tx)[1-\sigma(\theta^Tx)]}\sigma(\theta^Tx)[1-\sigma(\theta^Tx)]x_j \\
&amp;=[y-\sigma(\theta^Tx)]x_j \\
&amp;=\sum_{i=1}^n[y^{(i)}-\sigma(\theta^T\mathbf{x}^{(i)})]x_j^{(i)}
\end{align}
\]</span> Because the derivative of sums is the sum of derivatives, the gradient of theta is simply the sum of this term for each training datapoint.</p>
<h2 id="gradient-ascent-optimization">Gradient Ascent Optimization</h2>
<p>In the case of logistic regression we can’t solve for <span class="math inline">\(θ\)</span> mathematically. Instead we use a computer to chose <span class="math inline">\(θ\)</span>. To do so we employ an algorithm called <strong>gradient ascent</strong>.</p>
<p><strong>Gradient ascent</strong> claims that if you <strong><em>continuously take small steps in the direction of your gradient, you will eventually make it to a local maxima</em></strong>. In the case of Logistic Regression you can prove that the result will always be a <u>global maxima</u>. <span class="math display">\[
\begin{align}
\theta_j^{new} &amp;= \theta_j^{old}+\eta\cdot\frac{\partial ll(\theta^{old})}{\partial \theta_j^{old}} \\
&amp;=\theta_j^{old}+\eta\cdot \sum_{i=1}^n[y^{(i)}-\sigma(\theta^T\mathbf{x}^{(i)})]x_j^{(i)}
\end{align}
\]</span> Where <span class="math inline">\(η\)</span> is the magnitude of the step size that we take. If you keep updating <span class="math inline">\(θ\)</span> using the equation above you will converge on the best values of <span class="math inline">\(θ\)</span>!</p>
<h2 id="regularized-logistic-regression">Regularized Logistic Regression</h2>
<p>Cost Function: <span class="math display">\[
J(\theta)=-[\frac{1}{m}\sum_{i=1}^my^{(i)}\log{\sigma(\theta^Tx^{(i)})}+(1-y^{(i)})\log{(1-\sigma(\theta^Tx^{(i)}))}]+\frac{\lambda}{2m}\sum_{j=1}^n \theta_j^2
\]</span> The second sum, <span class="math inline">\(\sum_{j=1}^n \theta_j^2\)</span> <strong>means to explicitly exclude</strong> the bias term, <span class="math inline">\(\theta_0\)</span></p>
<ul>
<li>I.e. the <span class="math inline">\(θ\)</span> vector is indexed from 0 to n (holding n+1 values, <span class="math inline">\(\theta_0\)</span> through <span class="math inline">\(\theta_n\)</span>), and this sum explicitly skips <span class="math inline">\(\theta_0\)</span>, by running from 1 to n, skipping 0. Thus, when computing the equation, we should continuously update the two following equations: $$ <span class="math display">\[\begin{align}
\text{Gradient descent:} \\
\text{Repeat \{ } \\
&amp; \theta_0:=\theta_0-\alpha\frac{1}{m}\sum_{i=1}^m(\sigma(\theta^Tx^{(i)})-y^{(i)})x_0^{(i)} \\
&amp; \theta_j:=\theta_j-\alpha[\frac{1}{m}\sum_{i=1}^m(\sigma(\theta^Tx^{(i)})-y^{(i)})x_j^{(i)}+ \frac{\lambda}{m}\theta_j] \\
\text{\} } \\

\end{align}\]</span> $$</li>
</ul>
<h2 id="logistic-regression-v.s.-bernoulli">Logistic Regression v.s. Bernoulli</h2>
<p>Logistic regression assumes the response is <strong>conditionally Bernoulli distributed</strong>, given the values of the features <span class="math display">\[
y |X \sim Bernoulli(p=\sigma(\theta^Tx))
\]</span></p>
<h1 id="qa">Q&amp;A</h1>
<h5 id="what-is-a-logistic-function">What is a logistic function?</h5>
<p><span class="math display">\[
\sigma(z)=\frac{1}{1+e^{-z}} \in [0,1] \text{, where } z \in(-\infty,\infty)
\]</span></p>
<h5 id="why-sigmoid-function"><strong>Why sigmoid function?</strong></h5>
<p>One of the nice properties of logistic regression is that the sigmoid function outputs the conditional probabilities of the prediction, the class probabilities.</p>
<h5 id="why-not-linear-regression"><strong>Why Not Linear Regression?</strong></h5>
<p>Linear regression is not appropriate in the case of a qualitative response.</p>
<p><strong>Reason:</strong> there is no natural way to convert a qualitative response variable with more than two levels into a quantitative response that is ready for linear regression.</p>
<h5 id="logistic-regression-v.s.">Logistic regression v.s. ?</h5>
<p><strong>LR v.s. Linear Regression</strong></p>
<ul>
<li><p>LR fitting use maximum likelihood; linear regression use lease squares</p></li>
<li>LR - classification; linear - regression</li>
<li>Both under Generalized Linear Models
<ul>
<li>Linear: <span class="math inline">\(h_\theta(x)=E[y|x;\theta]=\mu=\theta^Tx\)</span></li>
<li>LR: <span class="math inline">\(h_\theta(x)=E[y|x;\theta]=\frac{1}{1+e^{-\theta^Tx}}\)</span></li>
</ul></li>
</ul>
<p><strong>LR v.s. SVM</strong></p>
<ul>
<li>Both classification models, both Discriminative Model(conditional models)</li>
<li>LR use cross-entropy; SVM use Hinge loss</li>
<li>SVM tries to finds the “best” margin (distance between the line and the support vectors) that separates the classes and this reduces the risk of error on the data, while logistic regression does not, instead it can have different decision boundaries with different weights that are near the optimal point.</li>
<li>The risk of overfitting is less in SVM, while Logistic regression is vulnerable to overfitting.</li>
</ul>
<h1 id="multiple-logistic-regression">Multiple Logistic Regression</h1>
<p>We now consider the problem of predicting a binary response using multiple predictors</p>
<p><strong>Log-odds (Logit)</strong></p>
<p><span class="math display">\[
\begin{align}
\log{\frac{p(X)}{1-p(X)}}=\beta_0+\sum_{i=1}^p\beta_iX
\end{align}
\]</span> where X = (X1, . . .,Xp) are p predictors</p>
<p><strong>Logistic function:</strong></p>
<p><span class="math display">\[
\begin{align}
p(X)=\frac{e^{\beta_0+\sum_{i=1}^p\beta_iX}}{1+e^{\beta_0+\sum_{i=1}^p\beta_iX}} \\
\frac{p(X)}{1-p(X)}=e^{\beta_0+\sum_{i=1}^p\beta_iX}
\end{align}
\]</span></p>
<h2 id="confounding">Confounding</h2>
<p>In single variable setting: <img src="./3.png" width="600"></p>
<p>In multiple variables setting: <img src="./4.png" width="600"></p>
<blockquote>
<p>How is it possible for student status to be associated with an increase in probability of default in Table 4.2 and a decrease in probability of default in Table 4.3?</p>
</blockquote>
<p><img src="./5.png" width="600"></p>
<ul>
<li>The positive coefficient for student in the single variable logistic regression : the overall student default rate is higher than the non-student default rate</li>
<li>The negative coefficient for student in the multiple logistic regression: for a fixed value of balance and income, a student is less likely to default than a non-student.</li>
</ul>
<p><strong>Reason</strong>:The variables <em>student</em> and <em>balance</em> are correlated.</p>
<p><strong>Intuition</strong>: A student is riskier than a non-student if no information about the student’s credit card balance is available. However, that student is less risky than a non-student with the same credit card balance! <span class="math display">\[
\text{NLL} = -\frac{1}{N} \left[
                \left(
                    \sum_{i=0}^N y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)
                \right) - R(\mathbf{b}, \gamma)
            \right]
\]</span></p>
<hr>
<p><strong>Ref:</strong></p>
<p>James, Gareth, et al. <em>An introduction to statistical learning</em>. Vol. 112. New York: springer, 2013.</p>
<p>Hastie, Trevor, et al. &quot;The elements of statistical learning: data mining, inference and prediction.&quot; <em>The Mathematical Intelligencer</em> 27.2 (2005): 83-85</p>
<p><a href="https://www.coursera.org/lecture/machine-learning/classification-wlPeP" target="_blank" rel="noopener">Coursera Machine Learning course</a></p>
<p>https://web.stanford.edu/class/archive/cs/cs109/cs109.1166/pdfs/40%20LogisticRegression.pdf</p>
<p>https://www.statisticssolutions.com/assumptions-of-logistic-regression/</p>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        
          
        
        <div class="post-tags">
          
            <a href="/tags/Logistic-Regression/" rel="tag"># Logistic Regression</a>
          
            <a href="/tags/Classification/" rel="tag"># Classification</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/posts/4df00c7b/" rel="next" title="Study Note: Linear Regression Part II - Potential Problems">
                <i class="fa fa-chevron-left"></i> Study Note: Linear Regression Part II - Potential Problems
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/posts/5b711fed/" rel="prev" title="Study Note: Linear Discriminant Analysis, ROC & AUC, Confusion Matrix">
                Study Note: Linear Discriminant Analysis, ROC & AUC, Confusion Matrix <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  
    <div class="comments" id="comments">
      
        <div id="gitment-container"></div>
      
    </div>

  



        </div>
        
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>
  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
              <p class="site-author-name" itemprop="name">Kamel Chehboun</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

            <nav class="site-state motion-element">
                <div class="site-state-item site-state-posts">
                  <a href="/DS_Blog/archives">
                    <span class="site-state-item-count">44</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
          
                <div class="site-state-item site-state-categories">
                      <a href="/DS_Blog/categories/">
                    <span class="site-state-item-count">8</span>
                    <span class="site-state-item-name">categories</span>
                  </a>
                </div>
                
                <div class="site-state-item site-state-tags">
                      <a href="/tags/">
                    <span class="site-state-item-count">34</span>
                    <span class="site-state-item-name">tags</span>
                  </a>
                </div>
              
            </nav>
            <div class="feed-link motion-element">
              <a href="/DS_Blog/atom.xml" rel="alternate">
                <i class="fa fa-rss"></i>
                RSS
              </a>
            </div>
            <div class="links-of-author motion-element">
                <span class="links-of-author-item">
                  <a href="https://github.com/kdsmel" title="GitHub &rarr; https://github.com/kdsmel" rel="noopener" target="_blank"><i class="fa fa-fw fa-github"></i></a>
                </span>
              </div>
          </div>
      </div>
</div>
  </aside>
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2021</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">Kamel Chehboun</span>

  
    <span class="post-meta-divider">|</span>
    <span class="post-meta-item-icon">
      <i class="fa fa-area-chart"></i>
    </span>
    <span title="Symbols count total">518k</span>
</div>
      </div>
    </footer>
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
      </div>
  </div>
<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>
  <script color="0,0,255" opacity="0.5" zindex="-1" count="99" src="//cdn.bootcss.com/canvas-nest.js/1.0.1/canvas-nest.min.js"></script>
  <script id="ribbon" size="300" alpha="0.6" zindex="-1" src="/lib/canvas-ribbon/canvas-ribbon.js"></script>
  <script src="//cdn.jsdelivr.net/jquery/2.1.3/jquery.min.js"></script>
  <script src="//cdn.jsdelivr.net/fastclick/1.0.6/fastclick.min.js"></script>
  <script src="//cdn.jsdelivr.net/npm/jquery-lazyload@1/jquery.lazyload.min.js"></script>
  <script src="/DS_Blog/lib/velocity/velocity.min.js?v=1.2.1"></script>
  <script src="/DS_Blog/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>
  <script src="/DS_Blog/lib/fancybox/source/jquery.fancybox.pack.js"></script>
  <script src="/DS_Blog/lib/three/three.min.js"></script>
  <script src="/DS_Blog/lib/three/three-waves.min.js"></script>
  <script src="/DS_Blog/lib/three/canvas_lines.min.js"></script>
  <script src="/DS_Blog/lib/three/canvas_sphere.min.js"></script>
  <script src="/DS_Blog/js/utils.js?v=7.1.2"></script>
  <script src="/DS_Blog/js/motion.js?v=7.1.2"></script>
  <script src="/DS_Blog/js/schemes/muse.js?v=7.1.2"></script>
  <script src="/DS_Blog/js/next-boot.js?v=7.1.2"></script><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config("");
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="custom_mathjax_source">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>

<script type="text/javascript" src="/DS_Blog/js/src/dynamic_bg.js"></script>
       
  
 
